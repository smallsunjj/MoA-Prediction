{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "### running platform: Kaggle Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,QuantileTransformer,PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.stats import skew,boxcox,boxcox_normmax,kurtosis\n",
    "#sns.set_context(\"paper\", font_scale = 1, rc={\"grid.linewidth\": 3})\n",
    "pd.set_option('display.max_rows', 100, 'display.max_columns', 900)\n",
    "from matplotlib.pyplot import cm\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import sys\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../input/lish-moa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "train_drug= pd.read_csv('../input/lish-moa/train_drug.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_features = [feature for feature in train_features.columns if feature.startswith('g-')]\n",
    "c_features = [feature for feature in train_features.columns if feature.startswith('c-')]\n",
    "other_features = [feature for feature in train_features.columns if feature not in g_features and \n",
    "                                                             feature not in c_features and \n",
    "                                                             feature not in train_targets_scored and\n",
    "                                                             feature not in train_targets_nonscored]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    \n",
    "    def __init__(self, features, targets): \n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "  \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train() #  set the model to training mode\n",
    "    final_loss = 0  # Initialise final loss to zero\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad() #every time we use the gradients to update the parameters, we need to zero the gradients afterwards\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device) #Sending data to GPU(cuda) if gpu is available otherwise CPU\n",
    "        outputs = model(inputs) #output \n",
    "        loss = loss_fn(outputs, targets) #loss function\n",
    "        loss.backward() #compute gradients(work its way BACKWARDS from the specified loss)\n",
    "        optimizer.step()  #gradient optimisation\n",
    "        scheduler.step() \n",
    "        \n",
    "        final_loss += loss.item() #Final loss\n",
    "        \n",
    "    final_loss /= len(dataloader) #average loss\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval() #  set the model to evaluation/validation mode\n",
    "    final_loss = 0 # Initialise validation final loss to zero\n",
    "    valid_preds = [] #Empty list for appending prediction\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device) #Sending data to GPU(cuda) if gpu is available otherwise CPU\n",
    "        outputs = model(inputs) #output\n",
    "        loss = loss_fn(outputs, targets) #loss calculation\n",
    "        \n",
    "        final_loss += loss.item() #final validation loss\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy()) # get CPU tensor as numpy array # cannot get GPU tensor as numpy array directly\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds) #concatenating predictions under valid_preds\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():  # need to use NO_GRAD to keep the update out of the gradient computation\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy()) \n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = [1500, 1250, 1000, 750]\n",
    "        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n",
    "        self.dropout2 = nn.Dropout(self.dropout_value[0])\n",
    "        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n",
    "\n",
    "        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n",
    "        self.dropout3 = nn.Dropout(self.dropout_value[1])\n",
    "        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n",
    "\n",
    "        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n",
    "        self.dropout4 = nn.Dropout(self.dropout_value[2])\n",
    "        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n",
    "\n",
    "        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n",
    "        self.dropout5 = nn.Dropout(self.dropout_value[3])\n",
    "        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.leaky_relu(self.dense3(x))\n",
    "\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.leaky_relu(self.dense4(x))\n",
    "\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.dense5(x)\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneScheduler:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "        self.epochs_per_step = 0\n",
    "        self.frozen_layers = []\n",
    "\n",
    "    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n",
    "        self.frozen_layers = []\n",
    "\n",
    "        model_new = Model(num_features, num_targets)\n",
    "        model_new.load_state_dict(model.state_dict())\n",
    "\n",
    "        # Freeze all weights\n",
    "        for name, param in model_new.named_parameters():\n",
    "            layer_index = name.split('.')[0][-1]\n",
    "\n",
    "            if layer_index == 5:\n",
    "                continue\n",
    "\n",
    "            param.requires_grad = False\n",
    "\n",
    "            # Save frozen layer names\n",
    "            if layer_index not in self.frozen_layers:\n",
    "                self.frozen_layers.append(layer_index)\n",
    "\n",
    "        self.epochs_per_step = self.epochs // len(self.frozen_layers)\n",
    "\n",
    "        # Replace the top layers with another ones\n",
    "        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n",
    "        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n",
    "        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n",
    "        model_new.to(DEVICE)\n",
    "        return model_new\n",
    "\n",
    "    def step(self, epoch, model):\n",
    "        if len(self.frozen_layers) == 0:\n",
    "            return\n",
    "\n",
    "        if epoch % self.epochs_per_step == 0:\n",
    "            last_frozen_index = self.frozen_layers[-1]\n",
    "            \n",
    "            # Unfreeze parameters of the last frozen layer\n",
    "            for name, param in model.named_parameters():\n",
    "                layer_index = name.split('.')[0][-1]\n",
    "\n",
    "                if layer_index == last_frozen_index:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            del self.frozen_layers[-1]  # Remove the last layer as unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 24\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "WEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\n",
    "MAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\n",
    "DIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\n",
    "PCT_START = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n",
    "    vc = train.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n",
    "    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n",
    "\n",
    "    for seed_id in range(SEEDS):\n",
    "        kfold_col = 'kfold_{}'.format(seed_id)\n",
    "        \n",
    "        # STRATIFY DRUGS 18X OR LESS\n",
    "        dct1 = {}\n",
    "        dct2 = {}\n",
    "\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.index[idxV].values}\n",
    "            dct1.update(dd)\n",
    "\n",
    "        # STRATIFY DRUGS MORE THAN 18X\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
    "            dct2.update(dd)\n",
    "\n",
    "        # ASSIGN FOLDS\n",
    "        train[kfold_col] = train.drug_id.map(dct1)\n",
    "        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n",
    "        train[kfold_col] = train[kfold_col].astype('int8')\n",
    "        \n",
    "    return train\n",
    "\n",
    "SEEDS = 7\n",
    "NFOLDS = 7\n",
    "DRUG_THRESH = 18\n",
    "\n",
    "train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold_id, seed_id):\n",
    "    seed_everything(seed_id)\n",
    "    \n",
    "    train_ = process_data(train)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    kfold_col = f'kfold_{seed_id}'\n",
    "    trn_idx = train_[train_[kfold_col] != fold_id].index\n",
    "    val_idx = train_[train_[kfold_col] == fold_id].index\n",
    "    \n",
    "    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n",
    "    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n",
    "    \n",
    "    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n",
    "        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n",
    "        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n",
    "        \n",
    "        train_dataset = MoADataset(x_train, y_train)\n",
    "        valid_dataset = MoADataset(x_valid, y_valid)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
    "                                                  steps_per_epoch=len(trainloader),\n",
    "                                                  pct_start=PCT_START,\n",
    "                                                  div_factor=DIV_FACTOR[tag_name], \n",
    "                                                  max_lr=MAX_LR[tag_name],\n",
    "                                                  epochs=EPOCHS)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n",
    "\n",
    "        oof = np.zeros((len(train), len(target_cols_now)))\n",
    "        best_loss = np.inf\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            if fine_tune_scheduler is not None:\n",
    "                fine_tune_scheduler.step(epoch, model)\n",
    "\n",
    "            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n",
    "\n",
    "            if np.isnan(valid_loss):\n",
    "                break\n",
    "            \n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                oof[val_idx] = valid_preds\n",
    "                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n",
    "\n",
    "        return oof\n",
    "\n",
    "    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n",
    "\n",
    "    pretrained_model = Model(num_features, num_all_targets)\n",
    "    pretrained_model.to(DEVICE)\n",
    "\n",
    "    # Train on scored + nonscored targets\n",
    "    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n",
    "\n",
    "    # Load the pretrained model with the best loss\n",
    "    pretrained_model = Model(num_features, num_all_targets)\n",
    "    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n",
    "    pretrained_model.to(DEVICE)\n",
    "\n",
    "    # Copy model without the top layer\n",
    "    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n",
    "\n",
    "    # Fine-tune the model on scored targets only\n",
    "    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n",
    "\n",
    "    # Load the fine-tuned model with the best loss\n",
    "    model = Model(num_features, num_targets)\n",
    "    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), num_targets))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed_id):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold_id in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold_id, seed_id)\n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = [0, 1, 2, 3, 4, 5, 6]\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "\n",
    "for i in range(len(target_cols)):\n",
    "    score += log_loss(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "print(\"CV log_loss: \", score / y_pred.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
